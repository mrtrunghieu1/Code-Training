{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import ast\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import tree\n",
    "\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from constant import path, parameter\n",
    "from lib import hypothesis\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainMaster\n",
    "train_master = TrainMaster()\n",
    "\n",
    "from lib import evaluate\n",
    "rules = evaluate.Rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = parameter.MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cv_filename(cv_file):\n",
    "    path_cvfile = os.path.join(path.DATA_PATH,cv_file)\n",
    "    cv_mat = scipy.io.loadmat(path_cvfile)\n",
    "    return cv_mat['cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_id(data,cv_file, models, classes, niters, nfolds):\n",
    "    arrErrSvm = []\n",
    "    arrErrTree = []\n",
    "    for i in range(niters):\n",
    "        for j in range(nfolds):\n",
    "            train_index = []\n",
    "            cv_test = process_cv_filename(cv_file)\n",
    "            KF = KFold(n_splits=10)\n",
    "            meta_index = 0\n",
    "            # Test index\n",
    "            test_index = cv_test[0][i*nfolds + j]\n",
    "            test_index = np.concatenate(([i-1 for i in test_index]))\n",
    "            # Train index\n",
    "            train_index.append([i for i in range(len(data)) if i not in test_index]) \n",
    "            train_index = np.asarray(train_index[0])\n",
    "            # ------------- Create metadata -----------------\n",
    "            meta_data = np.zeros((len(models) * len(classes), train_index.shape[0]))\n",
    "            meta_targets = np.zeros(len(train_index))\n",
    "            meta_prob_test = np.zeros((len(models) * len(classes), test_index.shape[0]))\n",
    "            # -----------------------------------------------\n",
    "            features_train = data[train_index][:,0:data.shape[1] - 1]\n",
    "            targets_train = data[train_index][:,-1]\n",
    "            \n",
    "            features_test = data[test_index][:, 0:data.shape[1] - 1]\n",
    "            targets_test = data[test_index][:,-1]\n",
    "            #------------- Training - Predict Prob -----------------\n",
    "            for i in range(len(models)):\n",
    "                clf = models[i].fit(features_train,targets_train)\n",
    "                predict_proba = clf.predict_proba(features_test)\n",
    "#                 print('====',predict_proba.transpose().shape)\n",
    "                num_classes = len(classes)\n",
    "                meta_prob_test[num_classes * i:num_classes * i + num_classes, :] = predict_proba.transpose()\n",
    "            meta_prob_test = meta_prob_test.transpose()\n",
    "            \n",
    "            # ------------- Training Phase K- Fold------------------\n",
    "#             print('-----------------features_train',features_train)\n",
    "            train_set = data[train_index]\n",
    "            for train_indices, test_indices in KF.split(train_set):                \n",
    "                for i,model in enumerate(models):\n",
    "                    features_train_KF = train_set[train_indices][:,0:data.shape[1] - 1]\n",
    "                    targets_train_KF = train_set[train_indices][:,-1]\n",
    "                    #-----------------------------------------------------\n",
    "                    features_test_KF = train_set[test_indices][:,0:data.shape[1] - 1]\n",
    "                    targets_test_KF = train_set[test_indices][:,-1]\n",
    "                    #-----------------------------------------------------\n",
    "                    learner = model \n",
    "                    learner.fit(features_train_KF,targets_train_KF)\n",
    "                    predict_probability = learner.predict_proba(features_test_KF)\n",
    "                    num_classes = len(classes)\n",
    "                    meta_data[num_classes * i:num_classes * i + num_classes, meta_index:meta_index + len(test_indices)] = predict_probability.transpose()\n",
    "                    \n",
    "                meta_targets[meta_index:meta_index + len(test_indices)] = targets_test_KF\n",
    "                meta_index += len(test_indices)\n",
    "            #Transpose the metadata\n",
    "            meta_data = meta_data.transpose()\n",
    "            # ---------------------- SVC Combining ------------------------\n",
    "            clf_svm = svm.SVC()\n",
    "            clf_svm.fit(meta_data, targets_train)\n",
    "            targets_svm_predict = clf_svm.predict(meta_prob_test)\n",
    "            mean_combining_svm_rule = rules.error_combining_rule(targets_svm_predict,targets_test)\n",
    "            arrErrSvm.append(mean_combining_svm_rule)\n",
    "            \n",
    "            # ---------------------- Decision Trees -----------------------\n",
    "            clf_tree = tree.DecisionTreeClassifier()\n",
    "            clf_tree.fit(meta_data, targets_train)\n",
    "            targets_tree_predict = clf_tree.predict(meta_prob_test)\n",
    "            mean_combining_tree_rule = rules.error_combining_rule(targets_tree_predict,targets_test)\n",
    "            arrErrTree.append(mean_combining_tree_rule)\n",
    "            \n",
    "    pickle_file1 = {'Dataset':cv_file,'arrErrSvm':arrErrSvm}\n",
    "    pickle_file2 = {'Dataset':cv_file,'arrErrTree':arrErrTree}\n",
    "    file_result1 = os.path.join(path.RESULT_PATH,\"sym_rule_{}.pickle\".format(cv_file))\n",
    "    file_result2 = os.path.join(path.RESULT_PATH,\"dtree_rule_{}.pickle\".format(cv_file))\n",
    "    pickle_out1 = open(file_result1,\"wb\")\n",
    "    pickle_out2 = open(file_result2,\"wb\")\n",
    "    pickle.dump(pickle_file1, pickle_out1)\n",
    "    pickle.dump(pickle_file2, pickle_out2)\n",
    "#     pickle_out.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(len(path.DATA_DAT)):\n",
    "# ------------------- Load Data ----------------------------------\n",
    "        load_data = os.path.join(path.DATA_PATH, path.DATA_DAT[i])\n",
    "        data = np.loadtxt(load_data, delimiter=',')\n",
    "        classes = np.unique(data[:,-1])\n",
    "        split_train_test_by_id(data=data, cv_file=path.CV_FILENAME[i],\n",
    "                               models=models,classes=classes,\n",
    "                               niters=3, nfolds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
