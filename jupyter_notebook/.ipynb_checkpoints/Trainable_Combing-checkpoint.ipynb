{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import ast\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from constant import path, parameter\n",
    "from lib import hypothesis\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainMaster\n",
    "train_master = TrainMaster()\n",
    "\n",
    "from lib import evaluate\n",
    "rules = evaluate.Rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = parameter.MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cv_filename(cv_file):\n",
    "    path_cvfile = os.path.join(path.DATA_PATH,cv_file)\n",
    "    cv_mat = scipy.io.loadmat(path_cvfile)\n",
    "    return cv_mat['cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_by_id(data,cv_file, models, classes, niters, nfolds):\n",
    "    arrErrSvm = []\n",
    "    \n",
    "    for i in range(niters):\n",
    "        for j in range(nfolds):\n",
    "            train_index = []\n",
    "            cv_test = process_cv_filename(cv_file)\n",
    "            KF = KFold(n_splits=10)\n",
    "            meta_index = 0\n",
    "            # Test index\n",
    "            test_index = cv_test[0][i*nfolds + j]\n",
    "            test_index = np.concatenate(([i-1 for i in test_index]))\n",
    "            # Train index\n",
    "            train_index.append([i for i in range(len(data)) if i not in test_index]) \n",
    "            train_index = np.asarray(train_index[0])\n",
    "            # ------------- Create metadata -----------------\n",
    "            meta_data = np.zeros((len(models) * len(classes), train_index.shape[0]))\n",
    "            meta_targets = np.zeros(len(train_index))\n",
    "            meta_prob_test = np.zeros((len(models) * len(classes), test_index.shape[0]))\n",
    "            \n",
    "            features_train = data[train_index][:,0:data.shape[1] - 1]\n",
    "            targets_train = data[train_index][:,-1]\n",
    "            \n",
    "            features_test = data[test_index][:, 0:data.shape[1] - 1]\n",
    "            targets_test = data[test_index][:,-1]\n",
    "            \n",
    "            #------------- Training - Predict Prob -----------------\n",
    "            for i in range(len(models)):\n",
    "                clf = models[i].fit(features_train,targets_train)\n",
    "                predict_proba = clf.predict_proba(features_test)\n",
    "                num_classes = len(classes)\n",
    "                meta_prob_test[num_classes * i:num_classes * i + num_classes, :] = predict_proba.transpose()\n",
    "            meta_prob_test = meta_prob_test.transpose()\n",
    "            \n",
    "            # ------------- Training Phase K- Fold------------------\n",
    "            for train_indices, test_indices in KF.split(features_train):                \n",
    "                for i,model in enumerate(models):\n",
    "                    features_train_KF = features_train[train_indices][:,0:data.shape[1] - 1]\n",
    "                    targets_train_KF = features_train[train_indices][:,-1]\n",
    "          \n",
    "                    train_master.training_phase(model, features_train=features_train_KF,\n",
    "                                                targets_train = targets_train_KF)\n",
    "                    model_path = os.path.join(path.MODEL_PATH,type(model).__name__)\n",
    "                    features_test_KF = features_train[test_indices][:,0:data.shape[1] - 1]\n",
    "                    targets_test_KF = features_train[test_indices][:,- 1]\n",
    "                    predict_probability = train_master.testing_phase(model_path,features_test=features_test_KF)\n",
    "                    meta_data[num_classes * i:num_classes * i + num_classes, meta_index:meta_index + len(test_indices)] = predict_probability.transpose()\n",
    "                meta_targets[meta_index:meta_index + len(test_indices)] = targets_test_KF\n",
    "                meta_index += len(test_indices)\n",
    "            #Transpose the metadata\n",
    "            meta_data = meta_data.transpose()\n",
    "            clf_svm = svm.SVC()\n",
    "            clf_svm.fit(meta_data, targets_train)\n",
    "            targets_predict = clf_svm.predict(meta_prob_test)\n",
    "            mean_combining_svm_rule = evaluate.error_combining_rule(targets_predict,targets_test)\n",
    "            arrErrSvm.append(mean_combining_svm_rule)\n",
    "            \n",
    "    pickle_file = {'Dataset':cv_file,'arrErrSvm':arrErrSvm}\n",
    "    file_result = os.path.join(path.RESULT_PATH,\"result_svm_{}.pickle\".format(cv_file))\n",
    "    pickle_out = open(file_result,\"wb\")\n",
    "    pickle.dump(pickle_file, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(len(path.DATA_DAT)):\n",
    "# ------------------- Load Data ----------------------------------\n",
    "        load_data = os.path.join(path.DATA_PATH, path.DATA_DAT[i])\n",
    "        data = np.loadtxt(load_data, delimiter=',')\n",
    "        classes = np.unique(data[:,-1])\n",
    "        split_train_test_by_id(data=data, cv_file=path.CV_FILENAME[i],\n",
    "                               models=models,classes=classes,\n",
    "                               niters=3, nfolds = 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Dump model LogisticRegression to success! -----\n",
      "load model success\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (203,63) into shape (2,63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-bb6f58369811>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         split_train_test_by_id(data=data, cv_file=path.CV_FILENAME[i],\n\u001b[0;32m      8\u001b[0m                                \u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                                niters=3, nfolds = 10)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-dd1ae80aae6c>\u001b[0m in \u001b[0;36msplit_train_test_by_id\u001b[1;34m(data, cv_file, models, classes, niters, nfolds)\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0mtargets_test_KF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[0mpredict_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_master\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_test_KF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0mmeta_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmeta_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mmeta_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmeta_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmeta_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets_test_KF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mmeta_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (203,63) into shape (2,63)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
