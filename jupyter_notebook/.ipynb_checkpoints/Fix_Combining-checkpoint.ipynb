{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import ast\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from constant import path, parameter\n",
    "from lib import hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainMaster\n",
    "train_master = TrainMaster()\n",
    "\n",
    "from lib import evaluate\n",
    "rules = evaluate.Rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = parameter.MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cv_filename(cv_file):\n",
    "    path_cvfile = os.path.join(path.DATA_PATH,cv_file)\n",
    "    cv_mat = scipy.io.loadmat(path_cvfile)\n",
    "    return cv_mat['cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a053d8e8a742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# ---------------- Writing Output -----------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mrules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrErrSum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrErrProd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrErrMin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrErrMax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cv_file' is not defined"
     ]
    }
   ],
   "source": [
    "def split_train_test_by_id(data,cv_file, models, classes, niters, nfolds):\n",
    "    arrErrSum = []\n",
    "    arrErrProd = []\n",
    "    arrErrMax= []\n",
    "    arrErrMin = []\n",
    "    \n",
    "    for i in range(niters):\n",
    "        for j in range(nfolds):\n",
    "            train_index = []\n",
    "            cv_test = process_cv_filename(cv_file)\n",
    "            # Test index\n",
    "            test_index = cv_test[0][i*nfolds + j]\n",
    "            test_index = np.concatenate(([i-1 for i in test_index]))\n",
    "            # Train index\n",
    "            train_index.append([i for i in range(len(data)) if i not in test_index]) \n",
    "            train_index = np.asarray(train_index[0])\n",
    "            # ------------- Create metadata -----------------\n",
    "            meta_proba = np.zeros((len(models) * len(classes), test_index.shape[0]))\n",
    "            # ------------- Training Phase ------------------\n",
    "            for i,model in enumerate(models):\n",
    "#                 meta_proba = np.zeros((len(models) * classes, features_test.shape[0]))\n",
    "                features_train = data[train_index][:,0:data.shape[1] - 1]\n",
    "                targets_train = data[train_index][:,-1]\n",
    "                train_master.training_phase(model, features_train=features_train,\n",
    "                                           targets_train=targets_train)\n",
    "                \n",
    "            # --------------- Testing Phase: ----------------\n",
    "                model_path = os.path.join(path.MODEL_PATH,type(model).__name__)\n",
    "                features_test = data[test_index][:,0:data.shape[1] - 1]\n",
    "                predict_probability = train_master.testing_phase(model_path,features_test=features_test)\n",
    "                num_classes = len(classes)    # Number class \n",
    "                meta_proba[num_classes * i:num_classes * i + num_classes, :] = predict_probability.transpose()\n",
    "                targets_test = data[test_index][:,-1]\n",
    "                \n",
    "            meta_proba = meta_proba.transpose()\n",
    "        # ---------------- Caculate Mean and Variance by Sum Rules --------------\n",
    "            combining_sum_rule = rules.combining_sum_rule(meta_proba=meta_proba, len_models=len(models))\n",
    "            targets_combining_sum_rule = rules.target(combining_sum_rule, classes=classes)\n",
    "            mean_combining_sum_rule = rules.error_combining_rule(targets_combining_sum_rule, targets_test)\n",
    "            arrErrSum.append(mean_combining_sum_rule)\n",
    "\n",
    "        # ---------------- Caculate Mean and Variance by Product Rules ----------\n",
    "            combining_product_rule = rules.combining_product_rule(meta_proba,len(models))\n",
    "            targets_combining_product_rule = rules.target(combining_product_rule, classes)\n",
    "            mean_combining_product_rule = rules.error_combining_rule(targets_combining_product_rule, targets_test)\n",
    "            arrErrProd.append(mean_combining_product_rule)\n",
    "\n",
    "        # ---------------- Caculate Mean and Variance by Max Rules ----------\n",
    "            combining_max_rule = rules.combining_max_min(meta_proba, len(models), parameter= 'max')\n",
    "            targets_combining_max_rule = rules.target(combining_max_rule, classes)\n",
    "            mean_combining_max_rule = rules.error_combining_rule(targets_combining_max_rule, targets_test)\n",
    "            arrErrMax.append(mean_combining_max_rule)\n",
    "\n",
    "        # ---------------- Caculate Mean and Variance by Min Rules ----------\n",
    "            combining_min_rule = rules.combining_max_min(meta_proba, len(models), parameter= 'min')\n",
    "            targets_combining_min_rule = rules.target(combining_min_rule, classes)\n",
    "            mean_combining_min_rule = rules.error_combining_rule(targets_combining_min_rule, targets_test)\n",
    "            arrErrMin.append(mean_combining_min_rule)\n",
    "\n",
    "        # ---------------- Writing Output -----------------------------------\n",
    "    rules.writer_output(cv_file, arrErrSum, arrErrProd, arrErrMin, arrErrMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(len(path.DATA_DAT)):\n",
    "# ------------------- Load Data ----------------------------------\n",
    "        load_data = os.path.join(path.DATA_PATH, path.DATA_DAT[i])\n",
    "        data = np.loadtxt(load_data, delimiter=',')\n",
    "        classes = np.unique(data[:,-1])\n",
    "        split_train_test_by_id(data=data, cv_file=path.CV_FILENAME[i],\n",
    "                               models=models,classes=classes,\n",
    "                               niters=3, nfolds = 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
