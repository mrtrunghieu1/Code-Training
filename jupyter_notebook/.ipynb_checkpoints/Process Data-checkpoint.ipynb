{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import ast\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from constant import path, parameter\n",
    "from lib import evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_dat = ['australian.dat','bupa.dat','glass.dat']\n",
    "cv_filename = ['cv_australian.mat','cv_bupa.mat','cv_glass.mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN,Naive Bayes,LogisticRegression algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models = [ LogisticRegression(random_state=0),GaussianNB(),neighbors.KNeighborsClassifier(n_neighbors=10,p = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_models(models,features_train,targets_train,features_test):\n",
    "    meta_proba = np.zeros((len(models) * 2, features_test.shape[0]))\n",
    "    for i in range(len(models)):\n",
    "        learner = models[i]\n",
    "        learner.fit(features_train,targets_train)\n",
    "        predictions_proba = learner.predict_proba(features_test)\n",
    "        meta_proba[2*i][:] = predictions_proba.T[0]\n",
    "        meta_proba[2*i + 1][:] = predictions_proba.T[1]\n",
    "    meta_proba = meta_proba.transpose()\n",
    "    return meta_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.MODEL_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def training_phase(models,features_train,targets_train):\n",
    "    for i in range(len(models)):\n",
    "        learner = models[i]\n",
    "        learner.fit(features_train,targets_train)\n",
    "#         pickle_out = open(\"dict_{}.pickle\".format(cv_file),\"wb\")\n",
    "        model_path = os.path.join(path.MODEL_PATH,type(learner).__name__)\n",
    "        pickle_out = open('{}.pickle'.format(model_path),\"wb\")\n",
    "        pickle.dump(learner, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                      weights='uniform')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = parameter.MODEL\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].fit([[1,2,3],[2,1,4],[1,1,0]],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Dump model LogisticRegression to success! -----\n",
      "----- Dump model GaussianNB to success! -----\n",
      "----- Dump model KNeighborsClassifier to success! -----\n"
     ]
    }
   ],
   "source": [
    "from train import TrainMaster\n",
    "train_master = TrainMaster()\n",
    "for s in models:\n",
    "    train_master.training_phase(model = s,features_train=[[1,2,3],[2,1,4],[1,1,0]],targets_train=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54152175 0.39872895 0.33780843 0.60394648 0.35850239 0.63073175]\n",
      " [0.6825092  0.8937595  0.87214547 0.220398   0.04268019 0.25928691]\n",
      " [0.61262267 0.23622209 0.94254647 0.4692196  0.42170965 0.41523484]\n",
      " [0.71619133 0.11879307 0.15935087 0.46141659 0.68019061 0.08568724]\n",
      " [0.0081002  0.38179384 0.87741736 0.08093596 0.71737675 0.16442789]\n",
      " [0.90750098 0.88167341 0.41186004 0.80517205 0.73014711 0.29000414]\n",
      " [0.99973194 0.96640253 0.14818585 0.70166128 0.09056392 0.87511675]\n",
      " [0.5476114  0.19811232 0.11478263 0.28411075 0.86699143 0.99418481]\n",
      " [0.07206965 0.81323832 0.28787329 0.35879684 0.40013193 0.05612491]\n",
      " [0.56715802 0.91617505 0.62558767 0.56440021 0.71183687 0.85642174]]\n",
      "[[0.38182274 0.25241045 0.32284673]\n",
      " [0.30096907 0.31214656 0.37714413]\n",
      " [0.36061409 0.21931058 0.45259377]\n",
      " [0.39253597 0.26632789 0.08167937]\n",
      " [0.02967872 0.36639019 0.34728175]\n",
      " [0.57089101 0.5372735  0.23395473]\n",
      " [0.56713107 0.35232215 0.34110087]\n",
      " [0.27724072 0.35503459 0.36965581]\n",
      " [0.14362216 0.40445675 0.11466607]\n",
      " [0.37718608 0.54267064 0.49400314]]\n",
      "[[3.27050154e-01 1.42945281e-01 2.13066502e-01]\n",
      " [1.50423662e-01 3.81458263e-02 2.26135908e-01]\n",
      " [2.87454565e-01 9.96171352e-02 3.91378137e-01]\n",
      " [3.30462561e-01 8.08019283e-02 1.36543364e-02]\n",
      " [6.55597287e-04 2.73890021e-01 1.44271889e-01]\n",
      " [7.30694417e-01 6.43751287e-01 1.19441119e-01]\n",
      " [7.01473186e-01 8.75211965e-02 1.29679921e-01]\n",
      " [1.55582287e-01 1.71761687e-01 1.14115147e-01]\n",
      " [2.58583626e-02 3.25402618e-01 1.61568618e-02]\n",
      " [3.20104105e-01 6.52167181e-01 5.35766882e-01]]\n",
      "[[0.60394648 0.39872895 0.63073175]\n",
      " [0.6825092  0.8937595  0.87214547]\n",
      " [0.61262267 0.42170965 0.94254647]\n",
      " [0.71619133 0.68019061 0.15935087]\n",
      " [0.08093596 0.71737675 0.87741736]\n",
      " [0.90750098 0.88167341 0.41186004]\n",
      " [0.99973194 0.96640253 0.87511675]\n",
      " [0.5476114  0.86699143 0.99418481]\n",
      " [0.35879684 0.81323832 0.28787329]\n",
      " [0.56715802 0.91617505 0.85642174]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10,6).reshape(10, 6)\n",
    "# matrix = np.hsplit(x, 3)\n",
    "# matrix\n",
    "# combining = np.zeros((10,2))\n",
    "# for i in range(3):\n",
    "#     combining += matrix[i] \n",
    "# print(combining)\n",
    "print(x)\n",
    "rules = evaluate.Rules()\n",
    "print(rules.combining_sum_rule(x, 2))\n",
    "print(rules.combining_product_rule(x, 2))\n",
    "print(rules.combining_max_min(x, 2, parameter ='max'))\n",
    "min_value = rules.combining_max_min(x, 2, parameter ='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.unique([1,2,3,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1         2         3\n",
      "0  0.541522  0.358502  0.337808\n",
      "1  0.220398  0.042680  0.259287\n",
      "2  0.469220  0.236222  0.415235\n",
      "3  0.461417  0.118793  0.085687\n",
      "4  0.008100  0.381794  0.164428\n",
      "5  0.805172  0.730147  0.290004\n",
      "6  0.701661  0.090564  0.148186\n",
      "7  0.284111  0.198112  0.114783\n",
      "8  0.072070  0.400132  0.056125\n",
      "9  0.564400  0.711837  0.625588\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(min_value,columns=targets)\n",
    "print(df)\n",
    "a = df.idxmax(axis=1)\n",
    "a\n",
    "for i in a :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = rules.target(min_value, targets)\n",
    "def error_combining_rule(target_combining_rule, target_test):\n",
    "    boolen_result = []\n",
    "    for i in range(len(target_combining_rule)):\n",
    "        result = 1 if target_combining_rule[i] != target_test[i] else 0\n",
    "        boolen_result.append(result)\n",
    "    mean_combining_rule = statistics.mean(boolen_result)\n",
    "#     variance_combining_rule = statistics.variance(boolen_result)\n",
    "    return mean_combining_rule\n",
    "error_combining_rule(a,[1,2,1,1,2,1,1,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculate Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_sum_product(matrix1, matrix2, matrix3):\n",
    "    #Combining Algorithms use Sum Rules\n",
    "    combining_sum_rule = matrix1 + matrix2 + matrix3\n",
    "    combining_sum_rule = combining_sum_rule * 1/3   #Mean elements in matrix\n",
    "    #Combining algorithms PRODUCT RULES\n",
    "    combining_product_rule =  matrix1 * matrix2 * matrix3 * 1/3\n",
    "    return combining_sum_rule,combining_product_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining algorithms MIN RULES\n",
    "#Function compare element row in matrix => Min value \n",
    "def min_element(value1, value2, value3):\n",
    "    min_value = min([value1, value2, value3])\n",
    "    return min_value \n",
    "\n",
    "def row_in_matrix(i,matrix_1, matrix_2, matrix_3):\n",
    "    min_value_class1 = min_element(value1=matrix_1[i][0], value2=matrix_2[i][0], value3= matrix_3[i][0])\n",
    "    min_value_class2 = min_element(value1=matrix_1[i][1], value2=matrix_2[i][1], value3= matrix_3[i][1])\n",
    "    return min_value_class1, min_value_class2\n",
    "    \n",
    "def min_rule(matrix_1, matrix_2, matrix_3):\n",
    "    combining_min_rule = np.zeros((matrix_1.shape))\n",
    "    \n",
    "    #Store variables to combining matrix\n",
    "    for i in range(len(matrix_1)):\n",
    "        combining_min_rule[i] = row_in_matrix(i, matrix_1=matrix_1,\n",
    "                                         matrix_2=matrix_2,matrix_3=matrix_3)\n",
    "    return combining_min_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining algorithms MAX RULES\n",
    "#Create variables to store combining matrix\n",
    "def max_element(value1, value2, value3):\n",
    "        max_value = max([value1, value2, value3])\n",
    "        return max_value \n",
    "#Function compare element row in matrix => Max value \n",
    "def row_in_matrix(i,matrix_1, matrix_2, matrix_3):\n",
    "        max_value_class1 = max_element(value1=matrix_1[i][0], value2=matrix_2[i][0], value3= matrix_3[i][0])\n",
    "        max_value_class2 = max_element(value1=matrix_1[i][1], value2=matrix_2[i][1], value3= matrix_3[i][1])\n",
    "        return max_value_class1, max_value_class2\n",
    "    \n",
    "def max_rule(matrix_1, matrix_2, matrix_3):\n",
    "    combining_max_rule = np.zeros((matrix_1.shape))\n",
    "    \n",
    "    #Store variables to combining matrix\n",
    "    for i in range(len(matrix_1)):\n",
    "        combining_max_rule[i] = row_in_matrix(i, matrix_1=matrix_1,\n",
    "                                             matrix_2=matrix_2,matrix_3=matrix_3)\n",
    "    return combining_max_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(combining_matrix):\n",
    "    targets_combining_algorithm = []\n",
    "    for row in combining_matrix:\n",
    "        result = 1 if row[0] > row[1] else 2\n",
    "        targets_combining_algorithm.append(result)\n",
    "    targets_combining_algorithm = np.asarray(targets_combining_algorithm)\n",
    "    return targets_combining_algorithm\n",
    "    \n",
    "def error_combining_rule(target_combining_rule, target_test):\n",
    "    boolen_result = []\n",
    "    for i in range(len(target_test)):\n",
    "        result = 1 if target_combining_rule[i] != target_test[i] else 0\n",
    "        boolen_result.append(result)\n",
    "    mean_combining_rule = statistics.mean(boolen_result)\n",
    "#     variance_combining_rule = statistics.variance(boolen_result)\n",
    "    return mean_combining_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_dat(filename_dat):\n",
    "    samples = []\n",
    "    data_set = [i.strip().split() for i in open(\"../data/\" + filename_dat).readlines()] \n",
    "    for sample in data_set:\n",
    "        res = ast.literal_eval(sample[0])\n",
    "        sample = list(res)\n",
    "        samples.append(sample)\n",
    "    samples = np.asarray(samples)\n",
    "    return samples\n",
    "\n",
    "def process_cv_filename(cv_file):\n",
    "    cv_mat = scipy.io.loadmat('../data/' + cv_file)\n",
    "    return cv_mat['cv']\n",
    "\n",
    "def split_train_test_by_id(data,cv_file, niters, nfolds):\n",
    "    arrErrSum = []\n",
    "    arrErrProd = []\n",
    "    arrErrMax= []\n",
    "    arrErrMin = []\n",
    "    for i in range(niters):\n",
    "        for j in range(nfolds):\n",
    "            train_index = []\n",
    "            cv_test = process_cv_filename(cv_file)\n",
    "            test_index = cv_test[0][i*nfolds + j]\n",
    "            test_index = np.concatenate(([i-1 for i in test_index]))\n",
    "#             print(\"LOOP:\",i*nfolds + j)\n",
    "            train_index.append([i for i in range(len(data)) if i not in test_index]) \n",
    "            train_index = np.asarray(train_index[0])\n",
    "#             print(data[train_index])\n",
    "#             print(data[test_index])\n",
    "            meta_proba = training_models(models,features_train=data[train_index][:,0:data.shape[1] - 1],\n",
    "                               targets_train=data[train_index][:,-1], features_test=data[test_index][:,0:data.shape[1] - 1])\n",
    "            predict_lr_proba = meta_proba[:,0:2]\n",
    "            predict_gnb_proba = meta_proba[:,2:4]\n",
    "            predict_knn_proba = meta_proba[:,4:6]\n",
    "            #Caculate Mean and Variance by Sum Rules:\n",
    "            combining_sum_rule,combining_product_rule = combining_sum_product(predict_lr_proba,predict_gnb_proba,\n",
    "                                                                             predict_knn_proba)\n",
    "            targets_combining_sum_rule = target(combining_sum_rule)\n",
    "            mean_combining_sum_rule = error_combining_rule(targets_combining_sum_rule, data[test_index][:,-1] )\n",
    "            arrErrSum.append(mean_combining_sum_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Product Rules:\n",
    "            targets_combining_product_rule = target(combining_product_rule)\n",
    "            mean_combining_product_rule = error_combining_rule(targets_combining_product_rule, data[test_index][:,-1] )\n",
    "            arrErrProd.append(mean_combining_product_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Min Rules:\n",
    "            combining_min_rule = min_rule(predict_lr_proba,predict_gnb_proba,predict_knn_proba)\n",
    "            targets_combining_min_rule = target(combining_min_rule)\n",
    "            mean_combining_min_rule = error_combining_rule(targets_combining_min_rule, data[test_index][:,-1] )\n",
    "            arrErrMin.append(mean_combining_min_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Max Rules:\n",
    "            combining_max_rule = max_rule(predict_lr_proba,predict_gnb_proba,predict_knn_proba)\n",
    "            targets_combining_max_rule = target(combining_max_rule)\n",
    "            mean_combining_max_rule = error_combining_rule(targets_combining_max_rule, data[test_index][:,-1] )\n",
    "            arrErrMax.append(mean_combining_max_rule)\n",
    "#     #Caculate mean and variance Total Sum Rule\n",
    "#     mean_sum = statistics.mean(arrErrSum)\n",
    "#     variance_sum = statistics.variance(arrErrSum)\n",
    "#     #Caculate mean and variance Total Product Rule\n",
    "#     mean_product = statistics.mean(arrErrProd)\n",
    "#     variance_product = statistics.variance(arrErrProd)\n",
    "#     #Caculate mean and variance Total Min Rule\n",
    "#     mean_min = statistics.mean(arrErrMin)\n",
    "#     variance_min = statistics.variance(arrErrMin)\n",
    "#     #Caculate mean and variance Total Max Rule\n",
    "#     mean_max = statistics.mean(arrErrMax)\n",
    "#     variance_max = statistics.variance(arrErrMax)\n",
    "    \n",
    "    pickle_file = {'Dataset':cv_file,'arrErrSum':arrErrSum,'arrErrProd':arrErrProd,\n",
    "                   'arrErrMin':arrErrMin,'arrErrMax':arrErrMax}\n",
    "#     pickle_file = {'Dataset':cv_file,'mean_sum':mean_sum,'variance_sum':variance_sum,\n",
    "#                    'mean_product':mean_product,'variance_product':variance_product,\n",
    "#                   'mean_min':mean_min,'variance_min':variance_min,\n",
    "#                   'mean_max':mean_max,'variance_max':variance_max}\n",
    "    print(pickle_file)\n",
    "    pickle_out = open(\"dict_{}.pickle\".format(cv_file),\"wb\")\n",
    "    pickle.dump(pickle_file, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fb1453f41f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data_dat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msplit_train_test_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mniters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dict_{}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dat' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = []\n",
    "for i in range(len(data_dat)):\n",
    "    data = process_data_dat(data_dat[i])\n",
    "    split_train_test_by_id(data,cv_filename[i],niters=3,nfolds=10)\n",
    "    pickle_in = open(\"dict_{}.pickle\".format(cv_filename[i]),\"rb\")\n",
    "    example_dict = pickle.load(pickle_in)\n",
    "    dataframe.append(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame.from_dict(dataframe)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rules(cv_file,arrErrSum,arrErrProd,arrErrMin,arrErrMax):\n",
    "    #Caculate mean and variance Total Sum Rule\n",
    "    mean_sum = statistics.mean(arrErrSum)\n",
    "    variance_sum = statistics.variance(arrErrSum)\n",
    "    #Caculate mean and variance Total Product Rule\n",
    "    mean_product = statistics.mean(arrErrProd)\n",
    "    variance_product = statistics.variance(arrErrProd)\n",
    "    #Caculate mean and variance Total Min Rule\n",
    "    mean_min = statistics.mean(arrErrMin)\n",
    "    variance_min = statistics.variance(arrErrMin)\n",
    "    #Caculate mean and variance Total Max Rule\n",
    "    mean_max = statistics.mean(arrErrMax)\n",
    "    variance_max = statistics.variance(arrErrMax)\n",
    "    dict_data = {'Dataset':cv_file,'mean_sum':mean_sum,'variance_sum':variance_sum,\n",
    "                   'mean_product':mean_product,'variance_product':variance_product,\n",
    "                  'mean_min':mean_min,'variance_min':variance_min,\n",
    "                  'mean_max':mean_max,'variance_max':variance_max}\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = []\n",
    "for i in range(len(cv_filename)):\n",
    "    dict_data.append(mean_rules(cv_filename[i],data['arrErrSum'][i],data['arrErrProd'][i],\n",
    "                               data['arrErrMin'][i],data['arrErrMax'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Bảng 1: Classification error của các fixed combining rule')\n",
    "data_1 = pd.DataFrame.from_dict(dict_data)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['arrErrSum'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculate Win, Equal, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_compare_error(array1, array2):\n",
    "#     win_result = []\n",
    "    win_result = 0\n",
    "    for i in range(len(array1)):\n",
    "#         win_result.append(1 if array1[i] < array2[i] else 0)\n",
    "        if (array1[i] < array2[i]):\n",
    "            win_result += 1\n",
    "    return win_result\n",
    "\n",
    "def equal_compare_error(array1, array2):\n",
    "    equal_result = 0\n",
    "    for i in range(len(array1)):\n",
    "        if (array1[i] == array2[i]):\n",
    "            equal_result += 1\n",
    "    return equal_result\n",
    "\n",
    "def loss_compare_error(array1, array2):\n",
    "    loss_result = 0\n",
    "    for i in range(len(array1)):\n",
    "        if (array1[i] > array2[i]):\n",
    "            loss_result += 1\n",
    "    return loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data['arrErrSum'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for i in range(len(cv_filename)):\n",
    "    # Sum vs Product\n",
    "    win_compare_sum_prod = win_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    print(win_compare_sum_prod)\n",
    "    equal_compare_sum_prod = equal_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    loss_compare_sum_prod = loss_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    # Sum vs Min\n",
    "    win_compare_sum_min = win_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    print(win_compare_sum_min)\n",
    "    equal_compare_sum_min = equal_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    loss_compare_sum_min = loss_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    # Sum vs Max\n",
    "    win_compare_sum_max = win_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    equal_compare_sum_max = equal_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    loss_compare_sum_max = loss_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    dict_result = {'Dataset':cv_filename[i],'win_sum_prod':win_compare_sum_prod,'equal_sum_prod':equal_compare_sum_prod,\n",
    "                  'loss_sum_prod':loss_compare_sum_prod,'win_sum_min':win_compare_sum_min,'equal_sum_min':equal_compare_sum_min,\n",
    "                  'loss_sum_min':loss_compare_sum_min,'win_sum_max':win_compare_sum_max,'equal_sum_max':equal_compare_sum_max,\n",
    "                  'loss_sum_max':loss_compare_sum_max}\n",
    "    print(dict_result)\n",
    "    final_results.append(dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "#Wilcoxon Sum vs Other Rules\n",
    "print(\"DATA: cv_australian.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrProd'][0],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrMin'][0],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrMax'][0],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA: cv_bupa.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrProd'][1],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrMin'][1],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrMax'][1],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA: cv_glass.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrProd'][2],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrMin'][2],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrMax'][2],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Bảng 2: Kết quả kiểm định')\n",
    "data_2 = pd.DataFrame.from_dict(final_results)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
