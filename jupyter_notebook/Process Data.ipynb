{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import ast\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from constant import path, parameter\n",
    "from lib import evaluate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_dat = ['australian.dat','bupa.dat','glass.dat']\n",
    "cv_filename = ['cv_australian.mat','cv_bupa.mat','cv_glass.mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN,Naive Bayes,LogisticRegression algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "models = [ LogisticRegression(random_state=0),GaussianNB(),neighbors.KNeighborsClassifier(n_neighbors=10,p = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_models(models,features_train,targets_train,features_test):\n",
    "    meta_proba = np.zeros((len(models) * 2, features_test.shape[0]))\n",
    "    for i in range(len(models)):\n",
    "        learner = models[i]\n",
    "        learner.fit(features_train,targets_train)\n",
    "        predictions_proba = learner.predict_proba(features_test)\n",
    "        meta_proba[2*i][:] = predictions_proba.T[0]\n",
    "        meta_proba[2*i + 1][:] = predictions_proba.T[1]\n",
    "    meta_proba = meta_proba.transpose()\n",
    "    return meta_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.MODEL_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def training_phase(models,features_train,targets_train):\n",
    "    for i in range(len(models)):\n",
    "        learner = models[i]\n",
    "        learner.fit(features_train,targets_train)\n",
    "#         pickle_out = open(\"dict_{}.pickle\".format(cv_file),\"wb\")\n",
    "        model_path = os.path.join(path.MODEL_PATH,type(learner).__name__)\n",
    "        pickle_out = open('{}.pickle'.format(model_path),\"wb\")\n",
    "        pickle.dump(learner, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                      weights='uniform')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = parameter.MODEL\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].fit([[1,2,3],[2,1,4],[1,1,0]],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Dump model LogisticRegression to success! -----\n",
      "----- Dump model GaussianNB to success! -----\n",
      "----- Dump model KNeighborsClassifier to success! -----\n"
     ]
    }
   ],
   "source": [
    "from train import TrainMaster\n",
    "train_master = TrainMaster()\n",
    "for s in models:\n",
    "    train_master.training_phase(model = s,features_train=[[1,2,3],[2,1,4],[1,1,0]],targets_train=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05589496 0.94687181 0.97429958 0.1546169  0.44804312 0.38185898]\n",
      " [0.65349487 0.0036347  0.43536583 0.04519436 0.59036256 0.75410967]\n",
      " [0.49737587 0.11013221 0.20512766 0.45245352 0.70064576 0.9250191 ]\n",
      " [0.82842017 0.43282412 0.62725884 0.62791817 0.74141828 0.59476616]\n",
      " [0.27190686 0.98472502 0.7507166  0.16854511 0.19263967 0.22657017]\n",
      " [0.62679222 0.80958703 0.59846635 0.75059907 0.88880203 0.56482244]\n",
      " [0.50525847 0.24630014 0.32523531 0.81286499 0.80999612 0.34062907]\n",
      " [0.64707641 0.17472161 0.48449952 0.02925583 0.79259653 0.86747539]\n",
      " [0.83226892 0.81129977 0.50593926 0.01893632 0.19203148 0.97438745]\n",
      " [0.83174778 0.22463236 0.16322267 0.8335913  0.25145816 0.78400437]]\n",
      "[[0.07017062 0.46497164 0.45205285]\n",
      " [0.23289641 0.19799909 0.39649183]\n",
      " [0.3166098  0.27025932 0.37671559]\n",
      " [0.48544612 0.39141413 0.40734167]\n",
      " [0.14681732 0.3924549  0.32576226]\n",
      " [0.45913043 0.56612969 0.38776293]\n",
      " [0.43937449 0.35209875 0.22195479]\n",
      " [0.22544408 0.32243938 0.4506583 ]\n",
      " [0.28373508 0.33444375 0.49344224]\n",
      " [0.55511303 0.15869684 0.31574235]]\n",
      "[[0.00864231 0.4242394  0.37204505]\n",
      " [0.02953428 0.00214579 0.32831358]\n",
      " [0.22503946 0.07716367 0.189747  ]\n",
      " [0.52018008 0.32090372 0.37307233]\n",
      " [0.04582857 0.1896971  0.17008999]\n",
      " [0.47046965 0.7195626  0.33802722]\n",
      " [0.41070692 0.19950216 0.1107846 ]\n",
      " [0.01893076 0.13848374 0.42029141]\n",
      " [0.01576011 0.1557951  0.49298087]\n",
      " [0.69333771 0.05648564 0.12796728]]\n",
      "[[0.1546169  0.94687181 0.97429958]\n",
      " [0.65349487 0.59036256 0.75410967]\n",
      " [0.49737587 0.70064576 0.9250191 ]\n",
      " [0.82842017 0.74141828 0.62725884]\n",
      " [0.27190686 0.98472502 0.7507166 ]\n",
      " [0.75059907 0.88880203 0.59846635]\n",
      " [0.81286499 0.80999612 0.34062907]\n",
      " [0.64707641 0.79259653 0.86747539]\n",
      " [0.83226892 0.81129977 0.97438745]\n",
      " [0.8335913  0.25145816 0.78400437]]\n",
      "[[0.05589496 0.44804312 0.38185898]\n",
      " [0.04519436 0.0036347  0.43536583]\n",
      " [0.45245352 0.11013221 0.20512766]\n",
      " [0.62791817 0.43282412 0.59476616]\n",
      " [0.16854511 0.19263967 0.22657017]\n",
      " [0.62679222 0.80958703 0.56482244]\n",
      " [0.50525847 0.24630014 0.32523531]\n",
      " [0.02925583 0.17472161 0.48449952]\n",
      " [0.01893632 0.19203148 0.50593926]\n",
      " [0.83174778 0.22463236 0.16322267]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10,6).reshape(10, 6)\n",
    "# matrix = np.hsplit(x, 3)\n",
    "# matrix\n",
    "# combining = np.zeros((10,2))\n",
    "# for i in range(3):\n",
    "#     combining += matrix[i] \n",
    "# print(combining)\n",
    "print(x)\n",
    "rules = evaluate.Rules()\n",
    "print(rules.combining_sum_rule(x, 2))\n",
    "print(rules.combining_product_rule(x, 2))\n",
    "print(rules.combining_max_min(x, 2, parameter ='max'))\n",
    "print(rules.combining_max_min(x, 2, parameter ='min'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculate Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_sum_product(matrix1, matrix2, matrix3):\n",
    "    #Combining Algorithms use Sum Rules\n",
    "    combining_sum_rule = matrix1 + matrix2 + matrix3\n",
    "    combining_sum_rule = combining_sum_rule * 1/3   #Mean elements in matrix\n",
    "    #Combining algorithms PRODUCT RULES\n",
    "    combining_product_rule =  matrix1 * matrix2 * matrix3 * 1/3\n",
    "    return combining_sum_rule,combining_product_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining algorithms MIN RULES\n",
    "#Function compare element row in matrix => Min value \n",
    "def min_element(value1, value2, value3):\n",
    "    min_value = min([value1, value2, value3])\n",
    "    return min_value \n",
    "\n",
    "def row_in_matrix(i,matrix_1, matrix_2, matrix_3):\n",
    "    min_value_class1 = min_element(value1=matrix_1[i][0], value2=matrix_2[i][0], value3= matrix_3[i][0])\n",
    "    min_value_class2 = min_element(value1=matrix_1[i][1], value2=matrix_2[i][1], value3= matrix_3[i][1])\n",
    "    return min_value_class1, min_value_class2\n",
    "    \n",
    "def min_rule(matrix_1, matrix_2, matrix_3):\n",
    "    combining_min_rule = np.zeros((matrix_1.shape))\n",
    "    \n",
    "    #Store variables to combining matrix\n",
    "    for i in range(len(matrix_1)):\n",
    "        combining_min_rule[i] = row_in_matrix(i, matrix_1=matrix_1,\n",
    "                                         matrix_2=matrix_2,matrix_3=matrix_3)\n",
    "    return combining_min_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining algorithms MAX RULES\n",
    "#Create variables to store combining matrix\n",
    "def max_element(value1, value2, value3):\n",
    "        max_value = max([value1, value2, value3])\n",
    "        return max_value \n",
    "#Function compare element row in matrix => Max value \n",
    "def row_in_matrix(i,matrix_1, matrix_2, matrix_3):\n",
    "        max_value_class1 = max_element(value1=matrix_1[i][0], value2=matrix_2[i][0], value3= matrix_3[i][0])\n",
    "        max_value_class2 = max_element(value1=matrix_1[i][1], value2=matrix_2[i][1], value3= matrix_3[i][1])\n",
    "        return max_value_class1, max_value_class2\n",
    "    \n",
    "def max_rule(matrix_1, matrix_2, matrix_3):\n",
    "    combining_max_rule = np.zeros((matrix_1.shape))\n",
    "    \n",
    "    #Store variables to combining matrix\n",
    "    for i in range(len(matrix_1)):\n",
    "        combining_max_rule[i] = row_in_matrix(i, matrix_1=matrix_1,\n",
    "                                             matrix_2=matrix_2,matrix_3=matrix_3)\n",
    "    return combining_max_rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(combining_matrix):\n",
    "    targets_combining_algorithm = []\n",
    "    for row in combining_matrix:\n",
    "        result = 1 if row[0] > row[1] else 2\n",
    "        targets_combining_algorithm.append(result)\n",
    "    targets_combining_algorithm = np.asarray(targets_combining_algorithm)\n",
    "    return targets_combining_algorithm\n",
    "    \n",
    "def error_combining_rule(target_combining_rule, target_test):\n",
    "    boolen_result = []\n",
    "    for i in range(len(target_test)):\n",
    "        result = 1 if target_combining_rule[i] != target_test[i] else 0\n",
    "        boolen_result.append(result)\n",
    "    mean_combining_rule = statistics.mean(boolen_result)\n",
    "#     variance_combining_rule = statistics.variance(boolen_result)\n",
    "    return mean_combining_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_dat(filename_dat):\n",
    "    samples = []\n",
    "    data_set = [i.strip().split() for i in open(\"../data/\" + filename_dat).readlines()] \n",
    "    for sample in data_set:\n",
    "        res = ast.literal_eval(sample[0])\n",
    "        sample = list(res)\n",
    "        samples.append(sample)\n",
    "    samples = np.asarray(samples)\n",
    "    return samples\n",
    "\n",
    "def process_cv_filename(cv_file):\n",
    "    cv_mat = scipy.io.loadmat('../data/' + cv_file)\n",
    "    return cv_mat['cv']\n",
    "\n",
    "def split_train_test_by_id(data,cv_file, niters, nfolds):\n",
    "    arrErrSum = []\n",
    "    arrErrProd = []\n",
    "    arrErrMax= []\n",
    "    arrErrMin = []\n",
    "    for i in range(niters):\n",
    "        for j in range(nfolds):\n",
    "            train_index = []\n",
    "            cv_test = process_cv_filename(cv_file)\n",
    "            test_index = cv_test[0][i*nfolds + j]\n",
    "            test_index = np.concatenate(([i-1 for i in test_index]))\n",
    "#             print(\"LOOP:\",i*nfolds + j)\n",
    "            train_index.append([i for i in range(len(data)) if i not in test_index]) \n",
    "            train_index = np.asarray(train_index[0])\n",
    "#             print(data[train_index])\n",
    "#             print(data[test_index])\n",
    "            meta_proba = training_models(models,features_train=data[train_index][:,0:data.shape[1] - 1],\n",
    "                               targets_train=data[train_index][:,-1], features_test=data[test_index][:,0:data.shape[1] - 1])\n",
    "            predict_lr_proba = meta_proba[:,0:2]\n",
    "            predict_gnb_proba = meta_proba[:,2:4]\n",
    "            predict_knn_proba = meta_proba[:,4:6]\n",
    "            #Caculate Mean and Variance by Sum Rules:\n",
    "            combining_sum_rule,combining_product_rule = combining_sum_product(predict_lr_proba,predict_gnb_proba,\n",
    "                                                                             predict_knn_proba)\n",
    "            targets_combining_sum_rule = target(combining_sum_rule)\n",
    "            mean_combining_sum_rule = error_combining_rule(targets_combining_sum_rule, data[test_index][:,-1] )\n",
    "            arrErrSum.append(mean_combining_sum_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Product Rules:\n",
    "            targets_combining_product_rule = target(combining_product_rule)\n",
    "            mean_combining_product_rule = error_combining_rule(targets_combining_product_rule, data[test_index][:,-1] )\n",
    "            arrErrProd.append(mean_combining_product_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Min Rules:\n",
    "            combining_min_rule = min_rule(predict_lr_proba,predict_gnb_proba,predict_knn_proba)\n",
    "            targets_combining_min_rule = target(combining_min_rule)\n",
    "            mean_combining_min_rule = error_combining_rule(targets_combining_min_rule, data[test_index][:,-1] )\n",
    "            arrErrMin.append(mean_combining_min_rule)\n",
    "            \n",
    "            #Caculate Mean and Variance by Max Rules:\n",
    "            combining_max_rule = max_rule(predict_lr_proba,predict_gnb_proba,predict_knn_proba)\n",
    "            targets_combining_max_rule = target(combining_max_rule)\n",
    "            mean_combining_max_rule = error_combining_rule(targets_combining_max_rule, data[test_index][:,-1] )\n",
    "            arrErrMax.append(mean_combining_max_rule)\n",
    "#     #Caculate mean and variance Total Sum Rule\n",
    "#     mean_sum = statistics.mean(arrErrSum)\n",
    "#     variance_sum = statistics.variance(arrErrSum)\n",
    "#     #Caculate mean and variance Total Product Rule\n",
    "#     mean_product = statistics.mean(arrErrProd)\n",
    "#     variance_product = statistics.variance(arrErrProd)\n",
    "#     #Caculate mean and variance Total Min Rule\n",
    "#     mean_min = statistics.mean(arrErrMin)\n",
    "#     variance_min = statistics.variance(arrErrMin)\n",
    "#     #Caculate mean and variance Total Max Rule\n",
    "#     mean_max = statistics.mean(arrErrMax)\n",
    "#     variance_max = statistics.variance(arrErrMax)\n",
    "    \n",
    "    pickle_file = {'Dataset':cv_file,'arrErrSum':arrErrSum,'arrErrProd':arrErrProd,\n",
    "                   'arrErrMin':arrErrMin,'arrErrMax':arrErrMax}\n",
    "#     pickle_file = {'Dataset':cv_file,'mean_sum':mean_sum,'variance_sum':variance_sum,\n",
    "#                    'mean_product':mean_product,'variance_product':variance_product,\n",
    "#                   'mean_min':mean_min,'variance_min':variance_min,\n",
    "#                   'mean_max':mean_max,'variance_max':variance_max}\n",
    "    print(pickle_file)\n",
    "    pickle_out = open(\"dict_{}.pickle\".format(cv_file),\"wb\")\n",
    "    pickle.dump(pickle_file, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fb1453f41f27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data_dat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msplit_train_test_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mniters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpickle_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dict_{}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dat' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = []\n",
    "for i in range(len(data_dat)):\n",
    "    data = process_data_dat(data_dat[i])\n",
    "    split_train_test_by_id(data,cv_filename[i],niters=3,nfolds=10)\n",
    "    pickle_in = open(\"dict_{}.pickle\".format(cv_filename[i]),\"rb\")\n",
    "    example_dict = pickle.load(pickle_in)\n",
    "    dataframe.append(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame.from_dict(dataframe)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rules(cv_file,arrErrSum,arrErrProd,arrErrMin,arrErrMax):\n",
    "    #Caculate mean and variance Total Sum Rule\n",
    "    mean_sum = statistics.mean(arrErrSum)\n",
    "    variance_sum = statistics.variance(arrErrSum)\n",
    "    #Caculate mean and variance Total Product Rule\n",
    "    mean_product = statistics.mean(arrErrProd)\n",
    "    variance_product = statistics.variance(arrErrProd)\n",
    "    #Caculate mean and variance Total Min Rule\n",
    "    mean_min = statistics.mean(arrErrMin)\n",
    "    variance_min = statistics.variance(arrErrMin)\n",
    "    #Caculate mean and variance Total Max Rule\n",
    "    mean_max = statistics.mean(arrErrMax)\n",
    "    variance_max = statistics.variance(arrErrMax)\n",
    "    dict_data = {'Dataset':cv_file,'mean_sum':mean_sum,'variance_sum':variance_sum,\n",
    "                   'mean_product':mean_product,'variance_product':variance_product,\n",
    "                  'mean_min':mean_min,'variance_min':variance_min,\n",
    "                  'mean_max':mean_max,'variance_max':variance_max}\n",
    "    return dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = []\n",
    "for i in range(len(cv_filename)):\n",
    "    dict_data.append(mean_rules(cv_filename[i],data['arrErrSum'][i],data['arrErrProd'][i],\n",
    "                               data['arrErrMin'][i],data['arrErrMax'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Bảng 1: Classification error của các fixed combining rule')\n",
    "data_1 = pd.DataFrame.from_dict(dict_data)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['arrErrSum'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caculate Win, Equal, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_compare_error(array1, array2):\n",
    "#     win_result = []\n",
    "    win_result = 0\n",
    "    for i in range(len(array1)):\n",
    "#         win_result.append(1 if array1[i] < array2[i] else 0)\n",
    "        if (array1[i] < array2[i]):\n",
    "            win_result += 1\n",
    "    return win_result\n",
    "\n",
    "def equal_compare_error(array1, array2):\n",
    "    equal_result = 0\n",
    "    for i in range(len(array1)):\n",
    "        if (array1[i] == array2[i]):\n",
    "            equal_result += 1\n",
    "    return equal_result\n",
    "\n",
    "def loss_compare_error(array1, array2):\n",
    "    loss_result = 0\n",
    "    for i in range(len(array1)):\n",
    "        if (array1[i] > array2[i]):\n",
    "            loss_result += 1\n",
    "    return loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data['arrErrSum'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for i in range(len(cv_filename)):\n",
    "    # Sum vs Product\n",
    "    win_compare_sum_prod = win_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    print(win_compare_sum_prod)\n",
    "    equal_compare_sum_prod = equal_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    loss_compare_sum_prod = loss_compare_error(data['arrErrSum'][i],data['arrErrProd'][i])\n",
    "    # Sum vs Min\n",
    "    win_compare_sum_min = win_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    print(win_compare_sum_min)\n",
    "    equal_compare_sum_min = equal_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    loss_compare_sum_min = loss_compare_error(data['arrErrSum'][i],data['arrErrMin'][i])\n",
    "    # Sum vs Max\n",
    "    win_compare_sum_max = win_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    equal_compare_sum_max = equal_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    loss_compare_sum_max = loss_compare_error(data['arrErrSum'][i],data['arrErrMax'][i])\n",
    "    dict_result = {'Dataset':cv_filename[i],'win_sum_prod':win_compare_sum_prod,'equal_sum_prod':equal_compare_sum_prod,\n",
    "                  'loss_sum_prod':loss_compare_sum_prod,'win_sum_min':win_compare_sum_min,'equal_sum_min':equal_compare_sum_min,\n",
    "                  'loss_sum_min':loss_compare_sum_min,'win_sum_max':win_compare_sum_max,'equal_sum_max':equal_compare_sum_max,\n",
    "                  'loss_sum_max':loss_compare_sum_max}\n",
    "    print(dict_result)\n",
    "    final_results.append(dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "#Wilcoxon Sum vs Other Rules\n",
    "print(\"DATA: cv_australian.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrProd'][0],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrMin'][0],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][0],data['arrErrMax'][0],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA: cv_bupa.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrProd'][1],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrMin'][1],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][1],data['arrErrMax'][1],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA: cv_glass.mat\")\n",
    "print(\"Wilcoxon Sum vs Product Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrProd'][2],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Min Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrMin'][2],zero_method='wilcox'))\n",
    "print(\"Wilcoxon Sum vs Max Rules:\",scipy.stats.wilcoxon(data['arrErrSum'][2],data['arrErrMax'][2],zero_method='wilcox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Bảng 2: Kết quả kiểm định')\n",
    "data_2 = pd.DataFrame.from_dict(final_results)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
